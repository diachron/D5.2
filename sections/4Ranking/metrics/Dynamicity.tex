%!TEX root = ../../../diachron-D5_2.tex

\subsubsection{Dynamicity Category}
\label{sec:Dynamicity} 


\paragraph{Currency Dimension}~\\ % dimension name 
Virtually all application domains are interested in getting access to data that is as up-to-date as possible. Actually, it is generally the case that data sources decrease in value as they become outdated. Therefore, the currency dimension is a key feature of Liked Data resources. The following metrics, intended to assess the currency of datasets, are based in the definition of this dimension provided by Kahn et al. (\cite{Kahn2002}) as "the degree to which information is up-to-date".

\paragraph{Currency of Documents/Statements Metric} ~\\ %metric name
% background for the metric
This metric is based in the definition of currency provided by Rula et al. (\cite{Rula2012}) as "the age of a value, where the age of a value is computed as the difference between the current time (the observation time) and the time when the value was last modified". The age of a dataset can be computed at both, the resource level (by comparing its last time of modification with the observation time) and the triples level (by comparing the value of last-modified statements instead).
% short description
\begin{mdframed}[style=metricdefinition]
\emph{Measures the degree to which data is up to date, by comparing the time when the data was observed (approximately the current time), with the time when the data (the document and each triple) was last modified.}
\end{mdframed}

% pseudocode
As listed in algorithm~\ref{alg:currencyDocsStm}, the metric is computed as the average of the comparisons of the observation time and the last modification time of each triple, normalized by the time elapsed since the publication of the document and the observation time. The last modification time is extracted from the properties: \textit{http://purl.org/dc/terms/modified} and \textit{http://semantic-mediawiki.org/swivt/1.0\#wikiPageModificationDate}. Likewise, the publication time of the document is extracted from \textit{http://purl.org/dc/terms/created}, \textit{http://purl.org/dc/terms/issued} and \textit{http://semantic-mediawiki.org/swivt/1.0\#creationDate}, as suggested by the study conducted by Rula et al. at \cite{Rula2012}.
\begin{algorithm}
\caption{Currency of Documents/Statements Algorithm} \label{alg:currencyDocsStm}
\begin{algorithmic}[1]
\Procedure{init}{}
\State accumTimeDiffs = 0;
\State countModifiedObjs = 0;
\State observationTime = getCurrentTime();
\State publishingTime = null;
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isLastModifiedTimeURI(?p)} 
\State accumTimeDiffs += (observationTime - parseAsTime(?o));
\State countModifiedObjs++;
\EndIf
\If {isPublishingTimeURI(?p)} 
\State publishingTime = parseAsTime(?o);
\EndIf ~\\
\Return{(countModifiedObjs-(accumTimeDiffs/(observationTime-publishingTime)))/countModifiedObjs;}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return the average of the difference between one and the ratio of the time elapsed since the last modification of each triple to the total time the dataset has been available. Since the former time span cannot be larger that the latter, the expected range is [0, 1]. The higher the value, the better, as it reflects how recently have the resources been updated. A value of 0 indicates that the resources have never been updated after their publication.

% end-of Currency of Documents/Statements Metric


\paragraph{Volatility Dimension}~\\ % dimension name 
%\subparagraph{Background for Volatility Quality Metrics} ~\\ %general background
In Linked Data evolution appeared almost at each new published version of data. Following the idea of \cite{TODS13} curators could define a list of changes that occur frequently and correspond to one or more low-level changes (added or deleted triples). These changes termed as Simple Changes also in the context of DIACHRON and comprise an upper abstract level of changes which is pilot-specific to describe group of changes that appear a special interest for each pilot. The detection of Simple Changes achieved accordingly to the methodology presented in \cite{TODS13} and followed in change detection service of DIACHRON (~\cite{D3.1}. The following three volatility metrics take into account these assumptions and background information.

\paragraph{Versions Volatility Metric} ~\\ %metric name
% background for the metric
The comparison of two sequential (or not) versions of datasets could contain a number of simple changes for each pilot. In other cases, it makes sense to compare an old version of a dataset with the newest one. 

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the number of simple changes happened accross two specified versions.}
\end{mdframed}

% pseudocode
The Versions Volatility Metric can be applied to a pair
of defined versions to count the detected number of Simple Changes. This achieved by querying the corresponding named graph where the the total number of Simple Changes have been stored which are returned as result. 

% algorithm
\begin{algorithm}
\caption{Versions Volatility Algorithm}
\begin{algorithmic}[1]
\Procedure{init}{}
\State numberOfChanges = 0
\EndProcedure
\Procedure{compute}{}
\State numberOfChanges = countSimpleChanges(v1,v2)
\EndProcedure
\State \Return {numberOfChanges}
\end{algorithmic}
\end{algorithm}

% metric value, range and rating
The metric will return the total number of Simple Changes between two versions [integer number].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Average Volatility Metric} ~\\ %metric name
% background for the metric
The number of detected simple changes could be varied accross different published versions for each curator/pilot. According to different scenarios some versions are similar while others appear many deltas. Thus, it is meaningful to examine all available published versions of datasets in order to find the average detected Simple Changes across each pair of versions. 

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the average number of simple changes detected across the published versions.}
\end{mdframed}

% pseudocode
The Average Volatility Metric firstly calculates the total number of published versions through a SPARQL query. Afterwards, it calculates the detected Simple Changes per versions pair and aggregate the sum of changes. Finally, it calculates and returns the ratio between aggregated sum and the number of examined pairs.


% algorithm
\begin{algorithm}
\caption{Average Volatility Metric Algorithm}
\begin{algorithmic}[1]

\Procedure{init}{}
\State changesTotal = 0
\State versionsNo = 0
\State retValue = 0
\EndProcedure
\Procedure{compute}{}
\State $ Versions [] = countVersions (SPARQL)$

\ForAll{$v[i],v[i+1] \in Versions$}
\State $changesTotal = changesTotal + countSimpleChanges(v[i],v[i+1])$
\EndFor
\EndProcedure
\State retValue = changesTotal / versionsNo -1
\State \Return {retValue}
\end{algorithmic}
\end{algorithm}

% metric value, range and rating
The metric will return the ratio [0..1] of average detected simple changes across the published dataset versions. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Weighted Volatility Metric} ~\\ %metric name
% background for the metric
In some applications pilots are interested more in evolution of specified versions. By applying a weighted sum model \cite{WSM} for each sequential pair of versions, we could adapt this preference for each pilot.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the average weighted sum of simple changes that has been detected across the published versions.}
\end{mdframed}

% pseudocode
The Weighted Volatility Metric after finding the total number of published versions it loads the weights
from the curator preference table. Afterwards, it calculates the simple changes per pair and multiply with the corresponding weight. Finally it calculates the ratio of weighted sum of changes to the examined pairs of versions.


% algorithm
\begin{algorithm}
\caption{Weighted Volatility Metric Algorithm}
\begin{algorithmic}[1]

\Procedure{init}{}
\State aggregSChanges = 0
\State versionsNo = 0
\State retValue = 0
\EndProcedure

\Procedure{LoadWeights}{}
\State $ weights [] = fetchWeights()$
\EndProcedure

\Procedure{compute}{}
\State $ versions [] = countVersions (SPARQL)$

\ForAll{$v[i],v[i+1] \in versions$}
	\ForAll{$w[j] \in weights$}
\State $changesTotal = changesTotal + w[j]*countSimpleChanges(v[i],v[i+1])$
	\EndFor
\EndFor
\EndProcedure
\State retValue = aggregSChanges / versionsNo -1
\State \Return {retValue}
\end{algorithmic}
\end{algorithm}

% metric value, range and rating
The metric will return the ratio of [0..1] aggregated weighted sum detected simple changes across the published dataset versions.


%%%%%%% Bibliography

%TODS13:Vicky Papavassiliou, Giorgos Flouris, Irini Fundulaki, Dimitris Kotzinos, Vassilis Christophides. High-Level Change Detection in RDF(S) KBs. Transactions on Database Systems (TODS), 38(1), 2013.%


%D3.1:Giorgos Flouris, Yannis Roussakis, Ioannis Chrysakis, Michalis Chortis, Kostas Stefanidis, Christos Pateritsas,Theodora Galani, Natalja Friesen, and Christoph Lange. D3.1: LOD evolution: change typology, updatelanguages and integrity rules. DIACHRON Deliverable D3.1, 2014.%


%WSM: Fishburn, P.C. (1967). "Additive Utilities with Incomplete Product Set: Applications to Priorities and Assignments". Operations Research Society of America (ORSA), Baltimore, MD, U.S.A.%

%Rula2012: Rula, A and Palmonari, M. and Maurino, A. Capturing the Age of Linked Open Data: Towards a Dataset-Independent Framework. Semantic Computing (ICSC), 2012 IEEE Sixth International Conference on, 2012.%

%Kahn2002: Kahn, Beverly K. and Strong, Diane M. and Wang, Richard Y. Information Quality Benchmarks: Product and Service Performance. Commun. ACM, 2002.%