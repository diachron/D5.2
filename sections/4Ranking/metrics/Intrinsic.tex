%!TEX root = ../../../diachron-D5_2.tex

\subsubsection{Intrinsic Category}
\label{sec:Intrinsic}

The intrinsic category metrics are independent of the user's context.
They reflect whether information presented in data correctly represent the real world and whether information is logically consistent itself.

\subsubsection{Accuracy} % dimension name 

Accuracy dimension metrics reflect the degree of correctness and precision with which the given dataset represent the real world facts.


\paragraph{Malformed Datatype Literals} ~\\ %metric name
\label{par:malformed}
% background for the metric
% short description
Literals that are incorrect regarding their data type are a very common problem.
Literals are nodes in an RDF graph, used to identify values such as numbers and dates.
The RDF specifies two types of literals: plain and typed.
A plain literal is a string combined with an optional language tag.
A typed literal comprises a string (the lexical form of the literal) and a datatype (identified by a URI) which is supposed to denote a mapping from lexical forms to some space of values.
In the Turtle syntax typed literals are notated with syntax such as: \textit{"13"^^xsd:int}
This Malformed Datatype Literals metric intends to check if the value of a typed literal is valid with regards to the given \textit{xsd} datatype.
The algorithm \ref{lst:malformedLiterals} describes the metric computation in more details.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of typed literals  not valid regarding its datatype to all literals}
\end{mdframed}

\begin{algorithm}
\caption{Malformed Datatype Literals Algorithm}\label{lst:malformedLiterals}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0 ;
\State malformedLiterals = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isLiteral(?o))} totalLiterals++ ;
\If {(isTypedLiteral(?o)) \&\& (!hasValidDatatype(?o))} malformedLiterals++ ; \EndIf
 \EndIf
\Return {malformedLiterals/totalLiterals}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the typed literals are considered by the metric. 
The metric values vary in the interval [0, 1], where the 0 indicates the best quality.

% pseudocode

% metric value, range and rating

\paragraph{Literals Incompatible with Datatype Range} ~\\ 
\label{par:incompartible}

Similar to the previous metric the 'literals incompatible with datatype range' metric verifies the correctness of literals regarding their datatype. 
Apart from typed literals described below a Literal datatype can also be defined through the predicate of a triple.
The range of attribute property (property corresponding to recourse with literal value) may be constrained to be a certain datatype.
Please see \ref{alg:dataRange} the computational algorithm for more details.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of literals incompatible with datatype range to all literals}
\end{mdframed}

% pseudocode

\begin{algorithm}
\caption{Literals Incompatible with datatype range}\label{alg:dataRange}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0
\State incompartibleLiterals = 0
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isLiteral(?o)} totalLiterals++ ;
\If {hasRange(?p) \&\& (literalDatatype(?o)!=rangeDatatype(?p)) } imcompartibleLiterals++; 
\EndIf
\EndIf
\Return {incompartibleLiterals/totalLiterals}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the literals referring by the property with the range characteristic are considered by the metric. 
The metric values vary in the interval [0, 1], where  the 0 indicates the best quality.


\paragraph{Defined Ontology Author} ~\\ 
\paragraph{POBO Definition Usage}~\\ 
\paragraph{Synonym Usage}~\\ 

\subsubsection{Consistency}~\\ 

Consistency metrics intend to identify any kinds of contradictions in data.

\paragraph{Entities As Members of Disjoint Classes}~\\


\paragraph{Homogeneous Datatypes}~\\

This metric deal with literals that conflict regarding their datatype.
Having restrictions to the literal datatype it's easy to validate the correctness of the data.
However, even if no restriction regarding literals is defined, different datatypes for a literals corresponding to the same property could point to inconsistences in the data.
In contrast to the previous metrics described in \ref{par:malformed} and in \ref{par:incompartible}, this metric deals only with literals which data type is  not defined. 
The metric computation therefore contains the following steps:
\begin{itemize}
\item Count frequency of different datatypes occurring with a particular predicates.
\item Identify properties corresponding to heterogeneous datatype literals.
\end{itemize}
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of properties containing heterogeneous datatype literals to all properties}
\end{mdframed}

% pseudocode

\begin{algorithm}
\caption{Homogeneous Datatypes}\label{lst:heterogeneous}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0;
\State heterogeneousDatatypeProperties = 0
\State  propertyMap = map(Property, List<Datatypes>)
\EndProcedure
\Procedure{analyzeProperties}{$\langle?s,?p,?o,?g\rangle$}
\If {isLiteral(?o)}  propertyMap.put(?p, Set.add.datatypeOf(?o)) \EndIf;
\EndProcedure
\Procedure{compute}{}
\ForAll {Properties in propertyMap}
\If {size(Set<Datatype>) > 1 } heterogeneousDatatypeProperties++; 
\EndIf
totalProperties ++;
\EndFor
\Return {eterogeneousDatatypeProperties/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], when the 0 indicates no properties containing heterogeneous datatype literals

If there are just a few triples of one predicate having a datatype different from all other triples, they are considered as outliers and are all reported.
In case if there is no obvious ratio of possibly wrong and possibly right triples the conflicting property will be reported to the user.
  
\paragraph{Misplaced Classes or Properties} ~\\

In some cases a URI that occurs in the predicate position of a triple is defined in the corresponding vocabulary as a class, or a contrariwise a URI in the object position is a property.
The common problem is e.g. the usage of property assigned to \textit{rdf:type} predicate.  
These kind of inconsistences make machine interpretation of the data more complex.
More details about the metric computation are presented in \ref{lst:misplacedCl}.


\begin{mdframed}[style=metricdefinition]
\emph{Number of properties in a class position + number of classes used as predicate to number of all classes and properties.}
\end{mdframed}


\begin{algorithm}
\caption{Misplaced Classes or Properties Metric Algorithm}\label{lst:misplacedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClassesUndProperties = 0 ;
\State misplacedClasses = 0 ;
\State misplacedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalClassesUndProperties++ ;\EndIf 
\If {!isProperty(?p)} misplacedProperties++ ; \EndIf 

\If {isURI(?o)} totalClassesUndProperties++; \EndIf  
\If {!isClass(?o)} misplacedClasses++ ; \EndIf 
\Return{(misplacedClasses+misplacedProperties)/totalClassesUndProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0, 1], where  the 0 indicates the best quality.


\paragraph{Misused Owl Datatype or Object Properties} ~\\

OWL language defines additional characteristics to some properties. 
If it's defined, a property can be either an instance of the \textit{owl:ObjectProperty} or \textit{owl:DatatypeProperty} class.
A datatype property relates some resource to a literal value, while an object property describes the relation between two resources.
Wrong usage of the datatype and object properties indicates inconsistences in the data.
The following algorithm  \ref{lst:misusedCl} presents more details about how the metric is computed.

\begin{mdframed}[style=metricdefinition]
\emph{Number of misused datatype and object properties to all properties}
\end{mdframed}


\begin{algorithm}
\caption{Misused Owl Datatype or Object Properties Metric Algorithm} \label{lst:misusedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State misusedObjectProperties = 0 ;
\State misusedDatatypeProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalProperties++ ;\EndIf 
\If {isDatatypeProperty(?p) \&\& isURI(?o)} misusedDatatypeProperties++ ; \EndIf 
\If {isObjectProperty(?p) \&\& isLiteral(?o)} misusedObjectProperties++ ; \EndIf 
\Return{(misusedObjectProperties + misusedDatatypeProperties)/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\paragraph{Obsolete Concepts in Ontology} ~\\
\paragraph{Ontology Hijacking}  ~\\

The 'ontology hijacking' term was first introduced in Hogan et. al~\cite{Hogan08} and is defined as 'the contribution of statements about classes and/or properties in a non-authoritative source such that reasoning on those classes and/or properties is affected'. 
In other words ontology hijacking refers to cases where external concepts are redefined in a local ontology.
Defining new super classes or properties of third-party classes or properties is an example for this problem, e.g. declaring \textit{rdfs:subPropertyOf} which is defined as a property,  to be a \textit{rdfs:Class}. 
The challenging question is how to define authoritative source.
We assume the ontologies/vocabularies published according to the best-practices as authoritative, while all other vocabularies as third parties documents.
More formal, a concept (class or property) is authoritative if it's not a blank node and if the corresponding vocabulary is retrievable.
To identify hijacked terms  we define a set of properties $P$ = {\textit{rdf:type}, \textit{rdf:domain}, \textit{rdf:range}, \textit{rdfs:subClassOf}, \textit{rdfs:subPropertyOf}\textit{owl:equivalentClass}, \textit{owl:equivalentProperty}, \textit{owl:inverseOf}, \textit{owl:onProperty}, \textit{owl:hasValue}, \textit{owl:someValuesFrom}, \textit{owl:allValuesFrom}, \textit{owl:intersectionOf}, \textit{owl:unionOf}, \textit{owl:maxCardinality}, \textit{owl:cardinality}, \textit{owl:oneOf} and a set of classes $C$ = {\textit{owl:FunctionalProperty}, \textit{owl:InverseFunctionalProperty}, \textit{owl:TransitiveProperty}, \textit{owl:SymmetricProperty}.
Metric check the two following cases:
\begin{itemize}
\item Classes in \textbf{C} appear in a position other than the object of a \textit{rdf:type} triple. This is the a property redefinition.
\item Properties in \textbf{P} appear in a position other than the predicate position.
\end{itemize}
Hogan et. al ~\cite{Hogan08} present more detail about the metric computation.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of hijacked triples to all triples}
\end{mdframed}


\begin{algorithm}
\caption{Ontology Hijacking Algorithm}\label{lst:undefCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State hijackedTriples = 0 ;
\State totalTriples = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(($?s \in \textbf{C}$)||($?o \in \textbf{C}$))\&\& ($?p== \textit{rdf:type}$)} hijackedTriples++ ;\EndIf
\If {($?p \in \textbf{P}$) \&\& isAuthority(?s) \&\& !isAuthority(?o)} hijackedTriples++; \EndIf
\Return{hijackedTriples/totalTriples}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Undefined Classes Metric} ~\\ 
\label{par:undefClass}

Oftentimes a terms which is used in the object position of a triple and is not a literal is not formally defined as being a class.
'Being defined' means that the term is defined either in some external ontology or at an earlier position in the given dataset. 
Regarding to Hogar ~\cite{hogan2010} to the most used undefined classes belong \texttt{foaf:UserGroup}, \texttt{rss:item}, \texttt{linkedct:link}, \texttt{politico:Term} .
The probability for undefined class in the subject position is very low, because the subject of a quad never references classes or properties in external vocabularies.
Therefore they is no need to analyze the subject for this metric.
For the most LOD data sets is sufficient to check object by the predicate \texttt{rdf:type}. 
In the case when LOD data set defines its own vocabulary the following predicates indicate that the object must be a defined class:  \texttt{rdfs:domain}, \texttt{rdfs:range}, \texttt{rdfs:subClassOf}, \texttt{owl:allValuesFrom}, \texttt{owl:someValuesFrom}, \texttt{owl:equivalentClass}, \texttt{owl:complementOf}, \texttt{owl:onClass}, \texttt{owl:disjointWith}.
The undefined classes problem occurs due to spelling or syntactic mistakes resolvable through minor fixes to the respective ontologies.
The missing classes should be defining in corresponding ontology or in a separate namespaces.


\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined classes to all classes in the object position in a dataset}
\end{mdframed}


\begin{algorithm}
\caption{Undefined Classes Metric Algorithm}\label{lst:undefCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClasses = 0 ;
\State undefinedClasses = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isClassProperty(?p) \&\& (isURI(?o))}  totalClasses++ ;\EndIf
\If {(!isDefinedClass(?o))  undefinedClasses++ }; \EndIf
\Return{undefinedClasses/totalClasses}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0, 1], where  the 0 indicates the best quality.


\paragraph{Undefined Properties}  ~\\ 


Similar to the Undefined Classes metric \ref{par:undefClass} the Undefined Properties metric identifies terms in the predicate position that are used without any formal definition.
 Hogan  ~\cite{hogan2010} identified the following properties that are often used without being defined:  \textit{foaf:image}, \textit{cycann:label} , \textit{foaf:tagLine}.
The following list of predicates indicate that the object of the quad must be a defined property:\textit{rdfs:subPropertyOf}, \textit{owl:onProperty}, \textit{owl:assertionProperty}, \textit{owl:equivalentProperty}, \textit{owl:propertyDisjointWith}.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined properties to all properties in the given data set}
\end{mdframed}

\begin{algorithm}
\caption{Undefined Properties Algorithm}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State undefinedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isURI(?p)} totalProperties++; \EndIf 
\If {!isDefined(?p)} udefinedProperties++; \EndIf

\If {isFromList(?p) \&\&  !isDefined(?o)} udefinedProperties++ ; \EndIf
\Return{undefinedProperties/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\subsubsection{Conciseness}

\paragraph{Duplicate Instance Metric} ~\\
% background for the metric
The information contained in Linked Data resources should not be redundant, which means that the instances contained in a dataset should, ideally, be unique. As stated by Yuangui Lei et al in \cite{Lei2007}, the mapping between the real-world objects described by the data sources and the instances contained in the semantic metadata should be one to one. That is, each statement about the existence of a real-world object should correspond to one and only one instance declaration.
Resources are divided into groups called classes. The members of a class are known as instances of the class. A triple of the form: R rdf:type C, states that C is an instance of rdfs:Class and R is an instance of C, as defined in the RDF Schema specification \footnote{$http://www.w3.org/TR/rdf-schema/\#ch\_type$}.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Computes the Duplicate Instance metric as one minus the ratio of the number of instances violating the uniqueness rule to the total number of instances in the dataset.}
\end{mdframed}

% pseudocode
As shown in algorithm~\ref{alg:duplicateInstance}, this metric is implemented by computing the subtraction one minus the ratio of the number of non-unique instances to the total number of declared instances. An instance is regarded as non-unique, if there is another instance declaration (i.e. rdf:type annotation) with its same subject URI and object value.
\begin{algorithm}
\caption{Duplicate Instance Algorithm} \label{alg:duplicateInstance}
\begin{algorithmic}[1]
\Procedure{init}{}
\State mapDeclaredInstances = new Map$\langle URI, Instance\rangle$();
\State countNonUniqueInstances = 0;
\State countTotalInstances = 0;
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {containsKey(mapDeclaredInstances, ?s)} 
countNonUniqueInstances++; 
\EndIf
\State countTotalInstances++;  ~\\
\Return{1.0 - (countNonUniqueInstances / countTotalInstances);}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return one minus the ratio of the number of unique instance declarations in the dataset, to the total number of instance declarations existing in the dataset. The expected range is [0..1], where 0 is the best rating (no duplicate instance declarations exist) and 1 is the worst rating (all instance declarations in the dataset are redundant).

% end-of Duplicate Instance Metric

\paragraph{Extensional Conciseness Metric} ~\\ 
% background for the metric
The conciseness of a dataset can be considered at the data level, as the redundancy of objects (i.e. instances) contained into the dataset. As defined by Mendes et al \cite{Mendes2012}, a dataset is concise (on the extensional or instance level), if it does not contain redundant objects, that is, objects being equivalent in their contents, yet having different identifiers.
Ideally, Linked Data resources should not contain redundant information, which implies that all the objects described by them should be unique. Uniqueness of objects is determined from their properties: one object is said to be unique if and only if there are no other objects with the same set of properties and corresponding values.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of the number of unique objects (i.e. instances) to the Total Number of objects. Two objects are equivalent if they have the same set of properties, all with the same values (but not necessarily the same ids).}
\end{mdframed}


% pseudocode
In the implementation of this metric (algorithm~\ref{alg:extConciseness}), objects are identified by their URI (the value of the subject attribute of the triples). The uniqueness of objects is determined from its properties: one object is said to be unique if and only if there is no other subject equivalent to it. Note that two equivalent objects may differ in their URI.
\begin{algorithm}
\caption{Extensional Conciseness Algorithm} \label{alg:extConciseness}
\begin{algorithmic}[1]
\Procedure{init}{}
\State mapDescribedObjs = new Map$\langle URI, Object\rangle$();
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\State curDescribedObj = getElementWithKey(mapDescribedObjs, ?s);
\State setObjectProperty(curDescribedObj, ?p, ?o); ~\\
\Return{countUniqueObjects(mapDescribedObjs)/countTotalObjects(mapDescribedObjs);}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return a ratio of the number of unique objects in the dataset (i.e. objects whose properties and their values are not duplicated in another object), to the total number of objects described in the dataset. The expected range is [0..1], where 0 is the worst rating (all objects are the same) and 1 is the best rating (all objects are unique).
% end-of Extensional Conciseness Metric

\paragraph{Ontology Versioning Conciseness}


