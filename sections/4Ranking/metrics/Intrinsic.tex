%!TEX root = ../../../diachron-D5_2.tex

\subsubsection{Intrinsic Category}
\label{sec:Intrinsic}

The intrinsic category metrics are independent of the user's context.
They reflect whether information presented in data correctly represent the real world and whether information is logically consistent itself.
 


\subsubsection{Accuracy} % dimension name 

Accuracy dimension metrics reflect the  degree of correctness and presicion with wich the given dataset represent the real world facts.


\paragraph{Malformed Datatype Literals} ~\\ %metric name
% background for the metric
% short description
Literals that are incorrect regarding their data type is a very common problem.
Literals are nodes in an RDF graph, used to identify values such as numbers and dates.
The RDF specifies two types of literals: plain and typed.
A plain literal is a string combined with an optional language tag.
A typed literal comprises a string (the lexical form of the literal) and a datatype (identified by a URI) which is supposed to denote a mapping from lexical forms to some space of values.
In the Turtle syntax typed literals are notated with syntax such as: \textit{"13"^^xsd:int}
This Malformed Datatype Literals metric intends to check if the value of a typed literal is valid with regards to the given \textit{xsd} datatype.
The algorithm \ref{lst:malformedLiterals} describes the metric computation in more details.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of typed literals that are not valid regarding its datatype to all literals}
\end{mdframed}

\begin{algorithm}
\caption{Malformed Datatype Literals Algorithm}\label{lst:malformedLiterals}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0 ;
\State malformedLiterals = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isLiteral(?o))} totalLiterals++ ;
\If {(isTypedLiteral(?o)) \&\& (!hasValidDatatype(?o))} malformedLiterals++ ; \EndIf
 \EndIf
\Return {malformedLiterals/totalLiterals}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the typed literals are considered by the metric. 
The metric values vary in the interval [0,1], where  the 0 indicates the best quality.

% pseudocode

% metric value, range and rating

\paragraph{Literals Incompartible with Datatype Range} ~\\ 

Similar to the previous metric the 'literals incompartible with datatype range' metric verifies the correctness of literals regarding their datatype. 
Apart from typed literals descibed below a Literal datatype can also be defined through the predicate of a triple.
The range of attribute property (property corresponding to ressourse with literal value) may be constrained to be a certain datatype.
Please see \ref{alg:dataRange} the computational algorithm for more details.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of literals incompartible with datatype range to all literals}
\end{mdframed}

% pseudocode

\begin{algorithm}
\caption{Literals Incompartible with datatype range}\label{alg:dataRange}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0
\State incompartibleLiterals = 0
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isLiteral(?o)} totalLiterals++ ;
\If {hasRange(?p) \&\& (literalDatatype(?o)!=rangeDatatype(?p)) } imcompartibleLiterals++; 
\EndIf
\EndIf
\Return {incompartibleLiterals/totalLiterals}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the literals referring by the property with the range characteristic are considered by the metric. 
The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\paragraph{Defined Ontology Author} ~\\ 
\paragraph{POBO Definition Usage}~\\ 
\paragraph{Synonym Usage}~\\ 



\subsubsection{Consistency}~\\ 

Consistency metrics intend to identify any kinds of contradictions in data.

\paragraph{Entities As Members of Disjoint Classes}~\\


\paragraph{Homogeneous Datatypes}~\\
 
\paragraph{Misplaced Classes or Properties} ~\\

In some cases a URI that occurs in the predicate position of a triple is defined in the corresponding vocabulary as a class, or a contrariwise a URI in the object position is a property.
The common problem is e.g. the usage of property assigned to \textit{rdf:type} predicate.  
These kind of inconsistences make machine interpretation of the data more complex.
More details about the metric computation are presented in \ref{alg:misplacedCl}.


\begin{mdframed}[style=metricdefinition]
\emph{Number of properties in a class position + number of classes used as predicate to number of all classes and properties.}
\end{mdframed}


\begin{algorithm}
\caption{Misplaced Classes or Properties Metric Algorithm}\label{alg:misplacedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClassesUndProperties = 0 ;
\State misplacedClasses = 0 ;
\State misplacedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalClassesUndProperties++ ;\EndIf 
\If {!isProperty(?p)} misplacedProperties++ ; \EndIf 

\If {isURI(?o)} totalClassesUndProperties++; \EndIf  
\If {!isClass(?o)} misplacedClasses++ ; \EndIf 
\Return{(misplacedClasses+misplacedProperties)/totalClassesUndProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\paragraph{Misused Owl Datatype or Object Properties} ~\\

OWL language defines additional characteristics to some properties. 
If it's defined, a property can be either an instance of the \textit{owl:ObjectProperty} or \textit{owl:DatatypeProperty} class.
A datatype property relates some resorce to a literal value, while an object property describe the relation between two resources.
Wrong usage of the datatype and object properties indicates inconsisnetces in the data.
The following algorithm  \ref{alg:misusedCl} presents more details about how the metric is computed.

\begin{mdframed}[style=metricdefinition]
\emph{Number of misused datatype and object properties to all properties}
\end{mdframed}


\begin{algorithm}
\caption{Misused Owl Datatype or Object Properties Metric Algorithm} \label{alg:misusedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State misusedObjectProperties = 0 ;
\State misusedDatatypeProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalProperties++ ;\EndIf 
\If {isDatatypeProperty(?p) \&\& isURI(?o)} misusedDatatypeProperties++ ; \EndIf 
\If {isObjectProperty(?p) \&\& isLiteral(?o)} misusedObjectProperties++ ; \EndIf 
\Return{(misusedObjectProperties + misusedDatatypeProperties)/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\paragraph{Obsolete Concepts in Ontology}
\paragraph{Ontology Hijacking}


\paragraph{Undefined Classes Metric} ~\\ 
\label{par:undefClass}

Oftentimes a terms which is used in the object position of a triple and is not a literal is not formaly defined as being a class.
'Being defined' means that the term is defined either in some external ontology or at an earlier position in the given dataset. 
Regarding to Hogar ~\cite{hogan2010} to the most used undefined classes belong \texttt{foaf:UserGroup}, \texttt{rss:item}, \texttt{linkedct:link}, \texttt{politico:Term} .
The probability for undefined class in the subject position is very low, because the subject of a quad never references classes or properties in external vocabularies.
Therefore they is no need to analyse the subject for this metric.
For the most LOD data sets is sufficient to check object by the predicate \texttt{rdf:type}. 
In the case when LOD data set defines its own vocabulary the following predicates indicate that the object must be a defined class:  \texttt{rdfs:domain}, \texttt{rdfs:range}, \texttt{rdfs:subClassOf}, \texttt{owl:allValuesFrom}, \texttt{owl:someValuesFrom}, \texttt{owl:equivalentClass}, \texttt{owl:complementOf}, \texttt{owl:onClass}, \texttt{owl:disjointWith}.
The undefined classes problem occurs due to spelling or syntactic mistakes resolvable through minor fixes to the respective ontologies.
The missing classes should be define in corresponding ontology or in a separate namenspace.


\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined classes to all classes in the object position in a dataset}
\end{mdframed}


\begin{algorithm}
\caption{Undefined Classes Metric Algorithm}\label{lst:undefCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClasses = 0 ;
\State undefinedClasses = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isClassProperty(?p) \&\& (isURI(?o))}  totalClasses++ ;\EndIf
\If {(!isDefinedClass(?o))  undefinedClasses++ }; \EndIf
\Return{undefinedClasses/totalClasses}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\paragraph{Undefined  Properties}  ~\\ 


Similar to the Undefined Classes metric \ref{par:undefClass} the Undefined Properties metric identifies terms in the predicate position that are used without any formal definition.
 Hogan  ~\cite{hogan2010} identified the following properties that are often used whithout being defined:  \textit{foaf:image}, \textit{cycann:label} , \textit{foaf:tagLine}.
The following list of predicates indicate that the object of the quad must be a defined property:\textit{rdfs:subPropertyOf}, \textit{owl:onProperty}, \textit{owl:assertionProperty}, \textit{owl:equivalentProperty}, \textit{owl:propertyDisjointWith}.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined properties to all properties in the given data set}
\end{mdframed}

\begin{algorithm}
\caption{Undefined Properties Algorithm}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State undefinedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isURI(?p)} totalProperties++; \EndIf 
\If {!isDefined(?p)} udefinedProperties++; \EndIf

\If {isFromList(?p) \&\&  !isDefined(?o)} udefinedProperties++ ; \EndIf
\Return{undefinedProperties/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary in the interval [0,1], where  the 0 indicates the best quality.


\subsubsection{Conciseness}

\paragraph{Duplicate Instance}

\paragraph{Extensional Conciseness Metric} ~\\ 
% background for the metric
The conciseness of a dataset can be considered at the data level, as the redundancy of objects (i.e. instances) contained into the dataset. As defined by Mendes et al \cite{Mendes2012}, a dataset is concise (on the extensional or instance level), if it does not contain redundant objects, that is, objects being equivalent in their contents, yet having different identifiers.
Ideally, Linked Data resources should not contain redundant information, which implies that all the objects described by them should be unique. Uniqueness of objects is determined from their properties: one object is said to be unique if and only if there are no other objects with the same set of properties and corresponding values.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of the number of unique objects (i.e. instances) to the Total Number of objects. Two objects are equivalent if they have the same set of properties, all with the same values (but not necessarily the same ids).}
\end{mdframed}

% pseudocode
In the implementation of this metric (algorithm~\ref{alg:extConciseness}), objects are identified by their URI (the value of the subject attribute of the triples). The uniqueness of objects is determined from its properties: one object is said to be unique if and only if there is no other subject equivalent to it. Note that two equivalent objects may differ in their URI.
\begin{algorithm}
\caption{Extensional Conciseness Algorithm} \label{alg:extConciseness}
\begin{algorithmic}[1]
\Procedure{init}{}
\State mapDescribedObjs = new Map$\langle URI, Object\rangle$();
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\State curDescribedObj = getElementWithKey(mapDescribedObjs, ?s);
\State setObjectProperty(curDescribedObj, ?p, ?o); ~\\
\Return{countUniqueObjects(mapDescribedObjs)/countTotalObjects(mapDescribedObjs);}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return a ratio of the number of unique objects in the dataset (i.e. objects whose properties and their values are not duplicated in another object), to the total number of objects described in the dataset. The expected range is [0..1], where 0 is the worst rating (all objects are the same) and 1 is the best rating (all objects are unique).

\paragraph{Ontology Versioning Conciseness}

