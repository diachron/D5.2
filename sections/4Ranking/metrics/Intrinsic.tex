%!TEX root = ../../../diachron-D5_2.tex

%~(required for: \textbf{GEN}, \textbf{DP}, \textbf{EBI})
\subsubsection{Intrinsic Category}
\label{sec:Intrinsic}
The intrinsic category metrics are independent of the user's context.
They reflect whether information presented in data correctly represent the real world and whether information is logically consistent itself.

\paragraph{Accuracy Dimension}~\\% dimension name 
Accuracy dimension metrics reflect the degree of correctness and precision with which the given dataset represent the real world facts.

\paragraph{Malformed Datatype Literals Metric}~(required for: \textbf{GEN}, \textbf{DP}, \textbf{EBI})~\\ %metric name
Literals in RDF are values, plain or typed, containing a textual string (eg. ``13''\string^\string^xsd:int).
It is common problem in LOD datasets that the literal value does not match its describing data type. 
The Malformed Datatype Literals metric intends to check this inconsistency.
The algorithm \ref{lst:malformedLiterals} describes the metric computation in more details.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of invalid typed literals against all literals}
\end{mdframed}

\begin{algorithm}
\caption{Malformed Datatype Literals Algorithm}\label{lst:malformedLiterals}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0 ;
\State malformedLiterals = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isLiteral(?o))} totalLiterals++ ;
\If {(isTypedLiteral(?o)) \&\& (!hasValidDatatype(?o))} malformedLiterals++ ; \EndIf
 \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the typed literals are considered by the metric. 
The metric values vary in the interval [0...1], where the 0 indicates the best quality.

\paragraph{Literals Incompatible with Datatype Range Metric}~(required for: \textbf{GEN}, \textbf{DP}, \textbf{EBI})~\\
Similar to the previously described metric, this verifies the correctness of literals regarding their datatype. 
By correctness, we mean that the data type attached to the literal is different than the one specified by the range of the used property in the ontology.
Algorithm~\ref{alg:dataRange} describes this metric.
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of incorrectly defined literals against all literals in the dataset.}
\end{mdframed}

\begin{algorithm}
\caption{Literals Incompatible with datatype range}\label{alg:dataRange}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalLiterals = 0
\State incompartibleLiterals = 0
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isLiteral(?o)} totalLiterals++ ;
\If {hasRange(?p) \&\& (literalDatatype(?o)!=rangeDatatype(?p)) } imcompartibleLiterals++; 
\EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Only the literals referring by the property with the range characteristic are considered by the metric. 
The metric values vary in the interval [0, 1], where  the 0 indicates the best quality.

\paragraph{Defined Ontology Author Metric$\star$}~(required for: \textbf{EBI})~\\ 
This metric is required of the Scientific Linked Data scenario as defined in Deliverable 5.1.
Imported ontologies should have common metadata which includes the definition of an author.

\begin{mdframed}[style=metricdefinition]
\emph{Checks whether the creator \texttt{efo:creator} is defined in the ontology}
\end{mdframed}

\begin{algorithm}
\caption{Defined Ontology Author Algorithm}
\begin{algorithmic}[1]
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {?o is owl:Ontology} ontologyList.add(?s); \EndIf
\If {?p is efo:creator \&\& ?s in ontologyList} definedCreator = 1 ; \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric will return a 1 (best rating) if the creator of the ontology is defined or 0 if otherwise.

\paragraph{POBO Definition Usage Metric$\star$}~(required for: \textbf{EBI})~\\ 
This metric is required of the Scientific Linked Data scenario as defined in Deliverable 5.1.
For this use case, it is required to check that each ontology class has a corresponding human readable definition.

\begin{mdframed}[style=metricdefinition]
\emph{Provides a measure for an ontology to check the usage of \texttt{pobo:def} in defined classes}
\end{mdframed}

\begin{algorithm}
\caption{POBO Definition Usage Algorithm}
\begin{algorithmic}[1]
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {?o is owl:Class} ontologyClassList.add(?s); \EndIf
\If {?p is pobo:def \&\& ?s in ontologyClassList} 
\State definitionHashMap.put(?s,?o);
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric will return a ratio of the number of classes without definitions against the total number defined classes. The expected range is [0..1], where 0 is the worst rating and 1 is the best rating

\paragraph{Synonym Usage Metric$\star$}~(required for: \textbf{EBI})~\\ 
This metric is required of the Scientific Linked Data scenario as defined in Deliverable 5.1.
Scientific terms usually have a number of synonyms.
Each ontology term should have at least one more synonym, thus in this metric we check if defined terms have a synonym attached to their description.

\begin{mdframed}[style=metricdefinition]
\emph{Measures the number of classes which has a synonym \texttt{efo:alternative\_term} described}
\end{mdframed}

\begin{algorithm}
\caption{Synonym Usage Algorithm}
\begin{algorithmic}[1]
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {?o is owl:Class} ontologyClassList.add(?s); \EndIf
\If {?p is efo:alternative\_term \&\& ?s in ontologyClassList} 
\State synonomHashMap.put(?s,?o);
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric will return a ratio of the number of classes without synonyms against the total number defined classes. The expected range is [0..1], where 0 is the worst rating and 1 is the best rating

\subsubsection{Consistency}~\\ 
Consistency metrics intend to identify any kinds of contradictions in data.
%\paragraph{Entities As Members of Disjoint Classes}~\\
\paragraph{Homogeneous Datatypes}~(required for: \textbf{GEN}, \textbf{DP}, \textbf{EBI})~\\
In some cases, the range of a property in a vocabulary is either (i) not defined; or (ii) given an open/generic type (such as \texttt{rdfs:Literal}).
This could cause inconsistencies in a dataset since values defined by the same property can be used with literals of different datatypes.
In contrast to the \emph{Malformed Literal} metric and \emph{Incompatible Datatype} metric, this metric deals  with the object in a triple where its property has an undefined/open range. 
The metric computation therefore contains the following steps:
\begin{itemize}
\item Count the frequency of different datatypes occurring with a particular predicates.
\item Identify the properties corresponding to heterogeneous datatype literals.
\end{itemize}

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of inconsistent object values in a dataset}
\end{mdframed}

% pseudocode
\begin{algorithm}
\caption{Homogeneous Datatypes}\label{lst:heterogeneous}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0;
\State heterogeneousDatatypeProperties = 0
\State  propertyMap = map(Property, List<Datatypes>)
\EndProcedure
\Procedure{analyzeProperties}{$\langle?s,?p,?o,?g\rangle$}
\If {isLiteral(?o)}  propertyMap.put(?p, Set.add.datatypeOf(?o)) \EndIf;
\EndProcedure
\Procedure{compute}{}
\ForAll {Properties in propertyMap}
\If {size(Set$\langle$Datatype$\rangle$) $\rangle$ 1 } heterogeneousDatatypeProperties++; 
\EndIf
totalProperties ++;
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary between [0...1], when the 0 indicates no properties containing heterogeneous datatype literals.
  
\paragraph{Misplaced Classes or Properties}~\\
In some cases a URI that occurs in the predicate position of a triple is defined in the corresponding vocabulary as a class, or a contrariwise a URI in the subject or object position is a defined property.
These kind of inconsistencies make machine interpretation of the data more complex.
More details about the metric computation are shown in \ref{lst:misplacedCl}.

\begin{mdframed}[style=metricdefinition]
\emph{Identifies the percentage of misplaced classes and properties in a dataset.}
\end{mdframed}

\begin{algorithm}
\caption{Misplaced Classes or Properties Metric Algorithm}\label{lst:misplacedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClassesAndProperties = 0 ;
\State misplacedClasses = 0 ;
\State misplacedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalClassesAndProperties++ ;\EndIf 
\If {!isProperty(?p)} misplacedProperties++ ; \EndIf 

\If {isURI(?o)} totalClassesAndProperties++; \EndIf  
\If {!isClass(?o)} misplacedClasses++ ; \EndIf 
\Return{(misplacedClasses+misplacedProperties)/totalClassesAndProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric values vary between  [0..1], where  the 0 indicates the best quality.

\paragraph{Misused Owl Datatype or Object Properties}~(required for: \textbf{EBI})~\\
OWL language defines additional characteristics to some properties. 
A property can be either an instance of the \textit{owl:ObjectProperty} or \textit{owl:DatatypeProperty} class.
A datatype property relates some resource to a literal value, while an object property describes a relation between two resources.
Wrong usage of the datatype and object properties indicates inconsistencies in the data.
The following algorithm  \ref{lst:misusedCl} presents more details about how the metric is computed.

\begin{mdframed}[style=metricdefinition]
\emph{Number of misused datatype and object properties to all properties}
\end{mdframed}

\begin{algorithm}
\caption{Misused Owl Datatype or Object Properties Metric Algorithm} \label{lst:misusedCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State misusedObjectProperties = 0 ;
\State misusedDatatypeProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {isURI(?p)} totalProperties++ ;\EndIf 
\If {isDatatypeProperty(?p) \&\& isURI(?o)} misusedDatatypeProperties++ ; \EndIf 
\If {isObjectProperty(?p) \&\& isLiteral(?o)} misusedObjectProperties++ ; \EndIf 
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric gives a value between [0..1], where  the 0 indicates the best quality.

\paragraph{Obsolete Concepts in Ontology$\star$}~(required for: \textbf{EBI})~\\
TODO TODO

\paragraph{Ontology Hijacking}~(required for: \textbf{EBI})~\\
\emph{Ontology Hijacking} was introduced by Hogan et. al~\cite{Hogan08} and is defined as ``the contribution of statements about classes and/or properties in a non-authoritative source such that reasoning on those classes and/or properties is affected''. 
In other words ontology hijacking refers to cases where external concepts are redefined in a local ontology.
Defining new super classes or properties of third-party classes or properties is an example for this problem, e.g. declaring \textit{rdfs:subPropertyOf} which is defined as a property, to be a \textit{rdfs:Class}. 
%The challenging question is how to define authoritative source.
%We assume the ontologies/vocabularies published according to the best-practices as authoritative, while all other vocabularies as third parties documents.
%More formal, a concept (class or property) is authoritative if it's not a blank node and if the corresponding vocabulary is retrievable.
To identify hijacked terms  we define a set of properties $P$:
\begin{itemize}
\item\textit{rdf:type}, \item\textit{rdf:domain}, \item\textit{rdf:range}, \item\textit{rdfs:subClassOf}, \item\textit{rdfs:subPropertyOf}\item\textit{owl:equivalentClass}, \item\textit{owl:equivalentProperty}, \item\textit{owl:inverseOf}, \item\textit{owl:onProperty}, \item\textit{owl:hasValue}, \item\textit{owl:someValuesFrom}, \item\textit{owl:allValuesFrom}, \item\textit{owl:intersectionOf}, \item\textit{owl:unionOf}, \item\textit{owl:maxCardinality}, \item\textit{owl:cardinality}, \item\textit{owl:oneOf};
\end{itemize}
and a set of classes $C$: 
\begin{itemize}
\item\textit{owl:FunctionalProperty}, \item\textit{owl:InverseFunctionalProperty}, \item\textit{owl:TransitiveProperty}, \item\textit{owl:SymmetricProperty}.
\end{itemize}
Metric check the two following cases:
\begin{itemize}
\item Classes in \textbf{C} appear in a position other than the object of a \textit{rdf:type} triple. 
\item Properties in \textbf{P} appear in a position other than the predicate position.
\end{itemize}

\begin{mdframed}[style=metricdefinition]
\emph{Checks for the re-definition of defined concepts breaking the consistency.}
\end{mdframed}

\begin{algorithm}
\caption{Ontology Hijacking Algorithm}\label{lst:undefCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State hijackedTriples = 0 ;
\State totalTriples = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(($?s \in \textbf{C}$) or ($?o \in \textbf{C}$))\&\& ($?p== \textit{rdf:type}$)} hijackedTriples++ ;\EndIf
\If {($?p \in \textbf{P}$) \&\& isAuthority(?s) \&\& !isAuthority(?o)} hijackedTriples++; \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Undefined Classes Metric}~(required for: \textbf{GEN})~\\ 
A class is defined in a vocabulary or dataset by the predicate \texttt{rdf:type} and an object value of \texttt{rdf:Class} or \texttt{owl:Class}.
In the case when LOD dataset defines its own vocabulary the following predicates indicate that the object must be a defined class:  \texttt{rdfs:domain}, \texttt{rdfs:range}, \texttt{rdfs:subClassOf}, \texttt{owl:allValuesFrom}, \texttt{owl:someValuesFrom}, \texttt{owl:equivalentClass}, \texttt{owl:complementOf}, \texttt{owl:onClass}, \texttt{owl:disjointWith}.
%'Being defined' means that the term is defined either in some external ontology or at an earlier position in the given dataset. 
%According to Hogan~\todo{CL: please add to bib}\cite{hogan2010} to the most used undefined classes belong \texttt{foaf:UserGroup}, \texttt{rss:item}, \texttt{linkedct:link}, \texttt{politico:Term} .
%The probability for undefined class in the subject position is very low, because the subject of a quad never references classes or properties in external vocabularies.
%Therefore they is no need to analyze the subject for this metric.
%For the most LOD datasets is sufficient to check object by the predicate \texttt{rdf:type}. 
The undefined classes problem occurs due to spelling or syntactic mistakes resolvable through minor fixes to the respective ontologies.
The missing classes should be defined in the dataset itself or an external ontology.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined classes to all classes in the object position in a dataset}
\end{mdframed}

\begin{algorithm}
\caption{Undefined Classes Metric Algorithm}\label{lst:undefCl}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalClasses = 0 ;
\State undefinedClasses = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isClassProperty(?p) \&\& (isURI(?o))}  totalClasses++ ;\EndIf
\If {(!isDefinedClass(?o))  undefinedClasses++ }; \EndIf
\Return{undefinedClasses/totalClasses}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Undefined Properties}~(required for: \textbf{GEN})~\\ 
Similar to the Undefined Classes metric \ref{par:undefClass} the Undefined Properties metric identifies terms in the predicate position that are used without any formal definition.
%The following list of predicates indicate that the object of the quad must be a defined property:\textit{rdfs:subPropertyOf}, \textit{owl:onProperty}, \textit{owl:assertionProperty}, \textit{owl:equivalentProperty}, \textit{owl:propertyDisjointWith}.

\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of undefined properties to all properties in the given data set}
\end{mdframed}

\begin{algorithm}
\caption{Undefined Properties Algorithm}
\begin{algorithmic}[1]
\Procedure{init}{}
\State totalProperties = 0 ;
\State undefinedProperties = 0 ;
\EndProcedure

\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {(isURI(?p)} totalProperties++; \EndIf 
\If {!isDefined(?p)} undefinedProperties++; \EndIf

\If {isFromList(?p) \&\&  !isDefined(?o)} undefinedProperties++ ; \EndIf
\Return{undefinedProperties/totalProperties}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The metric value ranges between [0..1], where  the 0 indicates the best quality, namely no undefined classes in the data set.

\paragraph{Conciseness Dimension}~\\ % dimension name 
High quality data sources should not contain redundant information, since it could lead to contradictions and in general would affect the clearness and manageability of data. Mendes et al. (\cite{Mendes2012}) identifies two levels of conciseness. Intensional, which refers to the schema level, that is, to the absence of redundant properties, and Extensional, referring to the extent to which instances (i.e. objects) described by the dataset, are unique.

\paragraph{Duplicate Instance Metric}~(required for: \textbf{GEN})~\\
% background for the metric
The information contained in Linked Data resources should not be redundant, which means that the instances contained in a dataset should, ideally, be unique. As stated by Yuangui Lei et al in \cite{Lei2007}, the mapping between the real-world objects described by the data sources and the instances contained in the semantic metadata should be one to one. That is, each statement about the existence of a real-world object should correspond to one and only one instance declaration.
Resources are divided into groups called classes. The members of a class are known as instances of the class. A triple of the form: R rdf:type C, states that C is an instance of rdfs:Class and R is an instance of C, as defined in the RDF Schema specification \footnote{\url{http://www.w3.org/TR/rdf-schema/#ch_type}}.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Computes the Duplicate Instance metric as one minus the ratio of the number of instances violating the uniqueness rule to the total number of instances in the dataset.}
\end{mdframed}

% pseudocode
As shown in algorithm~\ref{alg:duplicateInstance}, this metric is implemented by computing the subtraction one minus the ratio of the number of non-unique instances to the total number of declared instances. An instance is regarded as non-unique, if there is another instance declaration (i.e. rdf:type annotation) with its same subject URI and object value.
\begin{algorithm}
\caption{Duplicate Instance Algorithm} \label{alg:duplicateInstance}
\begin{algorithmic}[1]
\Procedure{init}{}
\State mapDeclaredInstances = new Map$\langle URI, Instance\rangle$();
\State countNonUniqueInstances = 0;
\State countTotalInstances = 0;
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\If {containsKey(mapDeclaredInstances, ?s)} 
countNonUniqueInstances++; 
\EndIf
\State countTotalInstances++;  ~\\
\Return{1.0 - (countNonUniqueInstances / countTotalInstances);}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return one minus the ratio of the number of unique instance declarations in the dataset, to the total number of instance declarations existing in the dataset. The expected range is [0..1], where 0 is the best rating (no duplicate instance declarations exist) and 1 is the worst rating (all instance declarations in the dataset are redundant).

% end-of Duplicate Instance Metric

\paragraph{Extensional Conciseness Metric}~(required for: \textbf{DP})~\\ 
% background for the metric
The conciseness of a dataset can be considered at the data level, as the redundancy of objects (i.e. instances) contained into the dataset. As defined by Mendes et al.~\cite{Mendes2012}, a dataset is concise (on the extensional or instance level), if it does not contain redundant objects, that is, objects being equivalent in their contents, yet having different identifiers.
Ideally, Linked Data resources should not contain redundant information, which implies that all the objects described by them should be unique. Uniqueness of objects is determined from their properties: one object is said to be unique if and only if there are no other objects with the same set of properties and corresponding values.

% short description
\begin{mdframed}[style=metricdefinition]
\emph{Calculates the ratio of the number of unique objects (i.e. instances) to the Total Number of objects. Two objects are equivalent if they have the same set of properties, all with the same values (but not necessarily the same ids).}
\end{mdframed}

% pseudocode
In the implementation of this metric (algorithm~\ref{alg:extConciseness}), objects are identified by their URI (the value of the subject attribute of the triples). The uniqueness of objects is determined from its properties: one object is said to be unique if and only if there is no other subject equivalent to it. Note that two equivalent objects may differ in their URI.
\begin{algorithm}
\caption{Extensional Conciseness Algorithm} \label{alg:extConciseness}
\begin{algorithmic}[1]
\Procedure{init}{}
\State mapDescribedObjs = new Map$\langle URI, Object\rangle$();
\EndProcedure
\Procedure{compute}{$\langle?s,?p,?o,?g\rangle$}
\State curDescribedObj = getElementWithKey(mapDescribedObjs, ?s);
\State setObjectProperty(curDescribedObj, ?p, ?o); ~\\
\Return{countUniqueObjects(mapDescribedObjs)/countTotalObjects(mapDescribedObjs);}
\EndProcedure
\end{algorithmic}
\end{algorithm}
% metric value, range and rating
The metric will return a ratio of the number of unique objects in the dataset (i.e. objects whose properties and their values are not duplicated in another object), to the total number of objects described in the dataset. The expected range is [0..1], where 0 is the worst rating (all objects are the same) and 1 is the best rating (all objects are unique).
% end-of Extensional Conciseness Metric

\paragraph{Ontology Versioning Conciseness}
TODO TODO TODO